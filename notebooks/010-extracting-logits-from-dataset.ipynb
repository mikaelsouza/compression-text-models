{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_NAME = \"compression-text-models\"\n",
    "\n",
    "curdir = os.path.abspath(os.path.curdir).split(\"/\")\n",
    "project_index = curdir.index(PROJECT_NAME)\n",
    "os.chdir(\"/\" + os.path.join(*curdir[:project_index + 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torchinfo\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_path = \"data/bin/tokenized-tensor.pt\"\n",
    "t = torch.load(tensor_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BertForMaskedLM                                         --\n",
       "├─BertModel: 1-1                                        --\n",
       "│    └─BertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                              22,881,792\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─Embedding: 3-3                              1,536\n",
       "│    │    └─LayerNorm: 3-4                              1,536\n",
       "│    │    └─Dropout: 3-5                                --\n",
       "│    └─BertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                             85,054,464\n",
       "├─BertOnlyMLMHead: 1-2                                  --\n",
       "│    └─BertLMPredictionHead: 2-3                        --\n",
       "│    │    └─BertPredictionHeadTransform: 3-7            592,128\n",
       "│    │    └─Linear: 3-8                                 22,911,586\n",
       "================================================================================\n",
       "Total params: 108,954,466\n",
       "Trainable params: 108,954,466\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    teacher_name,\n",
    ")\n",
    "\n",
    "teacher_model = transformers.BertForMaskedLM.from_pretrained(\n",
    "    teacher_name, \n",
    "    output_hidden_states=True,\n",
    ")\n",
    "teacher_model = teacher_model.to(DEVICE)\n",
    "teacher_model = teacher_model.eval()\n",
    "torchinfo.summary(teacher_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForMaskedLM                                   --\n",
       "├─GELUActivation: 1-1                                   --\n",
       "├─DistilBertModel: 1-2                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              22,881,792\n",
       "│    │    └─Embedding: 3-2                              (393,216)\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             42,527,232\n",
       "├─Linear: 1-3                                           590,592\n",
       "├─LayerNorm: 1-4                                        1,536\n",
       "├─Linear: 1-5                                           22,911,586\n",
       "├─CrossEntropyLoss: 1-6                                 --\n",
       "================================================================================\n",
       "Total params: 66,425,698\n",
       "Trainable params: 66,032,482\n",
       "Non-trainable params: 393,216\n",
       "================================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_configuration_path = \"configs/distillation/distilbert-base-cased.json\"\n",
    "extracted_base_model = \"models/artifacts/model-extraction/default-model.pth\"\n",
    "\n",
    "student_config = transformers.DistilBertConfig.from_pretrained(\n",
    "    student_configuration_path\n",
    ")\n",
    "student_config.output_hidden_states = True\n",
    "student_model = transformers.DistilBertForMaskedLM.from_pretrained(\n",
    "    extracted_base_model,\n",
    "    config=student_config,\n",
    ")\n",
    "student_model = student_model.to(DEVICE)\n",
    "student_model = student_model.train()\n",
    "torchinfo.summary(student_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLM Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 1.,  ..., 1., 1., 1.]), True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "mlm_smoothing = 0.7\n",
    "token_counts_path = \"data/processed/tokenized/separated-token-counts.pickle\"\n",
    "\n",
    "with open(token_counts_path, 'rb') as f:\n",
    "    token_counts = pickle.load(f)\n",
    "    counts_tensor = torch.LongTensor(token_counts)\n",
    "token_probs = torch.maximum(counts_tensor, torch.ones(counts_tensor.shape))\n",
    "token_probs = torch.pow(token_probs, -mlm_smoothing)\n",
    "token_probs[tokenizer.all_special_ids] = 0\n",
    "token_probs, token_probs.size(0) == tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Batch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = t[:16]\n",
    "example = example.to(DEVICE)\n",
    "batch = example, (example != 0).float()\n",
    "with torch.no_grad():\n",
    "    results = teacher_model(batch[0], attention_mask=batch[1])\n",
    "    t_logits, t_hidden_states = results['logits'], results['hidden_states']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare_batch_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100, 11433,  -100,  ...,  -100,  -100,  -100],\n",
       "        [ -100,  -100, 13859,  ...,  -100,  -100,  -100],\n",
       "        [ -100,  2627, 19747,  ...,  -100,  -100,  -100],\n",
       "        ...,\n",
       "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
       "        [ -100, 18116,  -100,  ...,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  2812,  ...,  -100,  -100,  -100]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "mlm_mask_prop = 0.15\n",
    "\n",
    "token_ids, attn_mask = batch\n",
    "token_ids = token_ids.to(DEVICE)\n",
    "attn_mask = attn_mask.to(DEVICE)\n",
    "\n",
    "lengths = attn_mask.sum(axis=-1)\n",
    "token_probs[token_ids.flatten()]\n",
    "\n",
    "mlm_labels = token_ids.clone().detach()\n",
    "\n",
    "bs, max_seq_len = token_ids.size()\n",
    "bs, max_seq_len\n",
    "\n",
    "x_prob = token_probs[token_ids.flatten()]\n",
    "n_tgt = math.ceil(mlm_mask_prop * lengths.sum().item())\n",
    "tgt_ids = torch.multinomial(x_prob / x_prob.sum(), n_tgt, replacement=False)\n",
    "pred_mask = torch.zeros(\n",
    "    bs * max_seq_len, dtype=torch.bool, device=token_ids.device\n",
    ") \n",
    "pred_mask[tgt_ids] = 1\n",
    "pred_mask = pred_mask.view(bs, max_seq_len)\n",
    "pred_mask[token_ids == tokenizer.pad_token_id] = 0\n",
    "word_mask, word_keep, word_rand = 0.8, 0.1, 0.1\n",
    "pred_probs = torch.tensor([word_mask, word_keep, word_rand], device=DEVICE)\n",
    "\n",
    "_token_ids_real = token_ids[pred_mask]\n",
    "_token_ids_rand = _token_ids_real.clone().random_(tokenizer.vocab_size)\n",
    "_token_ids_mask = _token_ids_real.clone().fill_(tokenizer.mask_token_id)\n",
    "probs = torch.multinomial(pred_probs, len(_token_ids_real), replacement=True)\n",
    "_token_ids = (\n",
    "    _token_ids_mask * (probs == 0).long()\n",
    "    + _token_ids_real * (probs == 1).long()\n",
    "    + _token_ids_rand * (probs == 2).long()\n",
    ")\n",
    "token_ids = token_ids.masked_scatter(pred_mask, _token_ids)\n",
    "\n",
    "mlm_labels[~pred_mask] = -100  # previously `mlm_labels[1-pred_mask] = -1`, cf pytorch 1.2.0 compatibility\n",
    "mlm_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_outputs = student_model(\n",
    "    input_ids=token_ids,\n",
    "    attention_mask=attn_mask,\n",
    ")\n",
    "\n",
    "teacher_outputs = teacher_model(\n",
    "    input_ids=token_ids,\n",
    "    attention_mask=attn_mask,\n",
    ")\n",
    "\n",
    "s_logits, s_hidden_states = student_outputs['logits'], student_outputs['hidden_states']\n",
    "t_logits, t_hidden_state = teacher_outputs['logits'], teacher_outputs['hidden_states']\n",
    "assert s_logits.size() == t_logits.size()\n",
    "\n",
    "mask = (mlm_labels > -1).unsqueeze(-1).expand_as(s_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 2.0\n",
    "alpha_ce = 0.33\n",
    "alpha_mlm = 0.33\n",
    "alpha_cos = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "ce_loss_fct = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "cosine_loss_fct = torch.nn.CosineEmbeddingLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_logits_slct = torch.masked_select(s_logits, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
    "s_logits_slct = s_logits_slct.view(-1, s_logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
    "t_logits_slct = torch.masked_select(t_logits, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
    "t_logits_slct = t_logits_slct.view(-1, s_logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
    "assert t_logits_slct.size() == s_logits_slct.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ce = (\n",
    "    ce_loss_fct(\n",
    "        torch.nn.functional.log_softmax(s_logits_slct / temperature, dim=-1),\n",
    "        torch.nn.functional.softmax(t_logits_slct / temperature, dim=-1),\n",
    "    ) * (temperature) ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mlm = lm_loss_fct(s_logits.view(-1, s_logits.size(-1)), mlm_labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_hidden_states_ = s_hidden_states[-1]  # (bs, seq_length, dim)\n",
    "t_hidden_states_ = t_hidden_states[-1]  # (bs, seq_length, dim)\n",
    "mask = attn_mask.unsqueeze(-1).expand_as(s_hidden_states_)  # (bs, seq_length, dim)\n",
    "assert s_hidden_states_.size() == t_hidden_states_.size()\n",
    "dim = s_hidden_states_.size(-1)\n",
    "\n",
    "s_hidden_states_slct = torch.masked_select(s_hidden_states_, mask.bool())  # (bs * seq_length * dim)\n",
    "s_hidden_states_slct = s_hidden_states_slct.view(-1, dim)  # (bs * seq_length, dim)\n",
    "t_hidden_states_slct = torch.masked_select(t_hidden_states_, mask.bool())  # (bs * seq_length * dim)\n",
    "t_hidden_states_slct = t_hidden_states_slct.view(-1, dim)  # (bs * seq_length, dim)\n",
    "\n",
    "target = s_hidden_states_slct.new(s_hidden_states_slct.size(0)).fill_(1)  # (bs * seq_length,)\n",
    "loss_cos = cosine_loss_fct(s_hidden_states_slct, t_hidden_states_slct, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_ce \u001b[39m*\u001b[39m alpha_ce \u001b[39m+\u001b[39m \\\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m        loss_mlm \u001b[39m*\u001b[39m alpha_mlm \u001b[39m+\u001b[39m \\\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m        loss_cos \u001b[39m*\u001b[39m alpha_cos\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/compression-text-models-fmgxBqLT-py3.8/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/compression-text-models-fmgxBqLT-py3.8/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss = loss_ce * alpha_ce + \\\n",
    "       loss_mlm * alpha_mlm + \\\n",
    "       loss_cos * alpha_cos\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.0\n",
    "learning_rate = 5e-4\n",
    "adam_epsilon = 1e-6\n",
    "\n",
    "warmup_prop = 0.05\n",
    "gradient_accumulation_steps = 50\n",
    "n_epoch = 1\n",
    "num_steps_epoch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in student_model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in student_model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon, betas=(0.9, 0.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "num_train_optimization_steps = (int(num_steps_epoch / gradient_accumulation_steps * n_epoch) + 1)\n",
    "warmup_steps = math.ceil(num_train_optimization_steps * warmup_prop)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=num_train_optimization_steps,\n",
    ")\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('compression-text-models-fmgxBqLT-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f100f66c9b41ceea0e3b5f026313cb1e6469ea04468aa1f9f63af1de251c63a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
