{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_NAME = \"compression-text-models\"\n",
    "\n",
    "curdir = os.path.abspath(os.path.curdir).split(\"/\")\n",
    "project_index = curdir.index(PROJECT_NAME)\n",
    "os.chdir(\"/\" + os.path.join(*curdir[:project_index + 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torchinfo\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "torch.cuda.set_per_process_memory_fraction(0.5)\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_path = \"data/bin/tokenized-tensor.pt\"\n",
    "t = torch.load(tensor_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BertForMaskedLM                                         --\n",
       "├─BertModel: 1-1                                        --\n",
       "│    └─BertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                              22,881,792\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─Embedding: 3-3                              1,536\n",
       "│    │    └─LayerNorm: 3-4                              1,536\n",
       "│    │    └─Dropout: 3-5                                --\n",
       "│    └─BertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                             85,054,464\n",
       "├─BertOnlyMLMHead: 1-2                                  --\n",
       "│    └─BertLMPredictionHead: 2-3                        --\n",
       "│    │    └─BertPredictionHeadTransform: 3-7            592,128\n",
       "│    │    └─Linear: 3-8                                 22,911,586\n",
       "================================================================================\n",
       "Total params: 108,954,466\n",
       "Trainable params: 108,954,466\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    teacher_name,\n",
    ")\n",
    "\n",
    "teacher_model = transformers.BertForMaskedLM.from_pretrained(\n",
    "    teacher_name, \n",
    "    output_hidden_states=True,\n",
    ")\n",
    "teacher_model = teacher_model.to(DEVICE)\n",
    "teacher_model = teacher_model.eval()\n",
    "torchinfo.summary(teacher_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForMaskedLM                                   --\n",
       "├─GELUActivation: 1-1                                   --\n",
       "├─DistilBertModel: 1-2                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              22,881,792\n",
       "│    │    └─Embedding: 3-2                              (393,216)\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             42,527,232\n",
       "├─Linear: 1-3                                           590,592\n",
       "├─LayerNorm: 1-4                                        1,536\n",
       "├─Linear: 1-5                                           22,911,586\n",
       "├─CrossEntropyLoss: 1-6                                 --\n",
       "================================================================================\n",
       "Total params: 66,425,698\n",
       "Trainable params: 66,032,482\n",
       "Non-trainable params: 393,216\n",
       "================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_configuration_path = \"configs/distillation/distilbert-base-cased.json\"\n",
    "extracted_base_model = \"models/artifacts/model-extraction/default-model.pth\"\n",
    "\n",
    "student_config = transformers.DistilBertConfig.from_pretrained(\n",
    "    student_configuration_path\n",
    ")\n",
    "student_config.output_hidden_states = True\n",
    "student_model = transformers.DistilBertForMaskedLM.from_pretrained(\n",
    "    extracted_base_model,\n",
    "    config=student_config,\n",
    ")\n",
    "student_model = student_model.to(DEVICE)\n",
    "student_model = student_model.train()\n",
    "torchinfo.summary(student_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLM Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 1.,  ..., 1., 1., 1.]), True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "mlm_smoothing = 0.7\n",
    "token_counts_path = \"data/processed/tokenized/separated-token-counts.pickle\"\n",
    "\n",
    "with open(token_counts_path, 'rb') as f:\n",
    "    token_counts = pickle.load(f)\n",
    "    counts_tensor = torch.LongTensor(token_counts)\n",
    "token_probs = torch.maximum(counts_tensor, torch.ones(counts_tensor.shape))\n",
    "token_probs = torch.pow(token_probs, -mlm_smoothing)\n",
    "token_probs[tokenizer.all_special_ids] = 0\n",
    "token_probs, token_probs.size(0) == tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Batch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = t[:1024]\n",
    "example = example.to(DEVICE)\n",
    "batch = example, (example != 0).float()\n",
    "#with torch.no_grad():\n",
    "#    results = teacher_model(batch[0], attention_mask=batch[1])\n",
    "#    t_logits, t_hidden_states = results['logits'], results['hidden_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([196., 178.,  75.,  ..., 512., 512., 512.], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1].sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare_batch_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100, 11433,  -100,  ...,  -100,  -100,  -100],\n",
       "        [ -100,  6438, 13859,  ...,  -100,  -100,  -100],\n",
       "        [ -100,  -100, 19747,  ...,  -100,  -100,  -100],\n",
       "        ...,\n",
       "        [ -100,  -100,  5560,  ...,  -100,  -100,  -100],\n",
       "        [ -100, 10319,  -100,  ..., 19110,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "mlm_mask_prop = 0.15\n",
    "\n",
    "token_ids, attn_mask = batch\n",
    "token_ids = token_ids.to(DEVICE)\n",
    "attn_mask = attn_mask.to(DEVICE)\n",
    "\n",
    "lengths = attn_mask.sum(axis=-1)\n",
    "token_probs[token_ids.flatten()]\n",
    "\n",
    "mlm_labels = token_ids.clone().detach()\n",
    "\n",
    "bs, max_seq_len = token_ids.size()\n",
    "bs, max_seq_len\n",
    "\n",
    "x_prob = token_probs[token_ids.flatten()]\n",
    "n_tgt = math.ceil(mlm_mask_prop * lengths.sum().item())\n",
    "tgt_ids = torch.multinomial(x_prob / x_prob.sum(), n_tgt, replacement=False)\n",
    "pred_mask = torch.zeros(\n",
    "    bs * max_seq_len, dtype=torch.bool, device=token_ids.device\n",
    ") \n",
    "pred_mask[tgt_ids] = 1\n",
    "pred_mask = pred_mask.view(bs, max_seq_len)\n",
    "pred_mask[token_ids == tokenizer.pad_token_id] = 0\n",
    "word_mask, word_keep, word_rand = 0.8, 0.1, 0.1\n",
    "pred_probs = torch.tensor([word_mask, word_keep, word_rand], device=DEVICE)\n",
    "\n",
    "_token_ids_real = token_ids[pred_mask]\n",
    "_token_ids_rand = _token_ids_real.clone().random_(tokenizer.vocab_size)\n",
    "_token_ids_mask = _token_ids_real.clone().fill_(tokenizer.mask_token_id)\n",
    "probs = torch.multinomial(pred_probs, len(_token_ids_real), replacement=True)\n",
    "_token_ids = (\n",
    "    _token_ids_mask * (probs == 0).long()\n",
    "    + _token_ids_real * (probs == 1).long()\n",
    "    + _token_ids_rand * (probs == 2).long()\n",
    ")\n",
    "token_ids = token_ids.masked_scatter(pred_mask, _token_ids)\n",
    "\n",
    "mlm_labels[~pred_mask] = -100  # previously `mlm_labels[1-pred_mask] = -1`, cf pytorch 1.2.0 compatibility\n",
    "mlm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 512]), torch.Size([1024, 512]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 47.46 GiB total capacity; 23.56 GiB already allocated; 14.51 GiB free; 23.73 GiB allowed; 23.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m student_outputs \u001b[39m=\u001b[39m student_model(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m teacher_outputs \u001b[39m=\u001b[39m teacher_model(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     input_ids\u001b[39m=\u001b[39mtoken_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39mattn_mask,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m s_logits, s_hidden_states \u001b[39m=\u001b[39m student_outputs[\u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m], student_outputs[\u001b[39m'\u001b[39m\u001b[39mhidden_states\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/compression-text-models-fmgxBqLT-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/compression-text-models-fmgxBqLT-py3.8/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:651\u001b[0m, in \u001b[0;36mDistilBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[39m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[39m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    649\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 651\u001b[0m dlbrt_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    652\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    653\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    654\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    655\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    656\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    657\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    658\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    659\u001b[0m )\n\u001b[1;32m    660\u001b[0m hidden_states \u001b[39m=\u001b[39m dlbrt_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    661\u001b[0m prediction_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_transform(hidden_states)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/compression-text-models-fmgxBqLT-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/compression-text-models-fmgxBqLT-py3.8/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:568\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    565\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    567\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 568\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[1;32m    570\u001b[0m     x\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m    571\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    576\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/compression-text-models-fmgxBqLT-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/compression-text-models-fmgxBqLT-py3.8/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:134\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    132\u001b[0m embeddings \u001b[39m=\u001b[39m word_embeddings \u001b[39m+\u001b[39m position_embeddings  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/compression-text-models-fmgxBqLT-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/compression-text-models-fmgxBqLT-py3.8/lib/python3.8/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/compression-text-models-fmgxBqLT-py3.8/lib/python3.8/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 47.46 GiB total capacity; 23.56 GiB already allocated; 14.51 GiB free; 23.73 GiB allowed; 23.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "student_outputs = student_model(\n",
    "    input_ids=token_ids,\n",
    "    attention_mask=attn_mask,\n",
    ")\n",
    "\n",
    "teacher_outputs = teacher_model(\n",
    "    input_ids=token_ids,\n",
    "    attention_mask=attn_mask,\n",
    ")\n",
    "\n",
    "s_logits, s_hidden_states = student_outputs['logits'], student_outputs['hidden_states']\n",
    "t_logits, t_hidden_state = teacher_outputs['logits'], teacher_outputs['hidden_states']\n",
    "assert s_logits.size() == t_logits.size()\n",
    "\n",
    "mask = (mlm_labels > -1).unsqueeze(-1).expand_as(s_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 2.0\n",
    "alpha_ce = 0.33\n",
    "alpha_mlm = 0.33\n",
    "alpha_cos = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "ce_loss_fct = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "cosine_loss_fct = torch.nn.CosineEmbeddingLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m s_logits_slct \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmasked_select(s_logits, mask)  \u001b[39m# (bs * seq_length * voc_size) modulo the 1s in mask\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m s_logits_slct \u001b[39m=\u001b[39m s_logits_slct\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, s_logits\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))  \u001b[39m# (bs * seq_length, voc_size) modulo the 1s in mask\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m t_logits_slct \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmasked_select(t_logits, mask)  \u001b[39m# (bs * seq_length * voc_size) modulo the 1s in mask\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's_logits' is not defined"
     ]
    }
   ],
   "source": [
    "s_logits_slct = torch.masked_select(s_logits, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
    "s_logits_slct = s_logits_slct.view(-1, s_logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
    "t_logits_slct = torch.masked_select(t_logits, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
    "t_logits_slct = t_logits_slct.view(-1, s_logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
    "assert t_logits_slct.size() == s_logits_slct.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_logits_slct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m loss_ce \u001b[39m=\u001b[39m (\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     ce_loss_fct(\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(s_logits_slct \u001b[39m/\u001b[39m temperature, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(t_logits_slct \u001b[39m/\u001b[39m temperature, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     ) \u001b[39m*\u001b[39m (temperature) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 's_logits_slct' is not defined"
     ]
    }
   ],
   "source": [
    "loss_ce = (\n",
    "    ce_loss_fct(\n",
    "        torch.nn.functional.log_softmax(s_logits_slct / temperature, dim=-1),\n",
    "        torch.nn.functional.softmax(t_logits_slct / temperature, dim=-1),\n",
    "    ) * (temperature) ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mlm = lm_loss_fct(s_logits.view(-1, s_logits.size(-1)), mlm_labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_hidden_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m s_hidden_states_ \u001b[39m=\u001b[39m s_hidden_states[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m t_hidden_states_ \u001b[39m=\u001b[39m t_hidden_states[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247505550227d/home/mikael/compression-text-models/notebooks/010-extracting-logits-from-dataset.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mexpand_as(s_hidden_states_)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's_hidden_states' is not defined"
     ]
    }
   ],
   "source": [
    "s_hidden_states_ = s_hidden_states[-1]  # (bs, seq_length, dim)\n",
    "t_hidden_states_ = t_hidden_states[-1]  # (bs, seq_length, dim)\n",
    "mask = attn_mask.unsqueeze(-1).expand_as(s_hidden_states_)  # (bs, seq_length, dim)\n",
    "assert s_hidden_states_.size() == t_hidden_states_.size()\n",
    "dim = s_hidden_states_.size(-1)\n",
    "\n",
    "s_hidden_states_slct = torch.masked_select(s_hidden_states_, mask.bool())  # (bs * seq_length * dim)\n",
    "s_hidden_states_slct = s_hidden_states_slct.view(-1, dim)  # (bs * seq_length, dim)\n",
    "t_hidden_states_slct = torch.masked_select(t_hidden_states_, mask.bool())  # (bs * seq_length * dim)\n",
    "t_hidden_states_slct = t_hidden_states_slct.view(-1, dim)  # (bs * seq_length, dim)\n",
    "\n",
    "target = s_hidden_states_slct.new(s_hidden_states_slct.size(0)).fill_(1)  # (bs * seq_length,)\n",
    "loss_cos = cosine_loss_fct(s_hidden_states_slct, t_hidden_states_slct, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_ce * alpha_ce + \\\n",
    "       loss_mlm * alpha_mlm + \\\n",
    "       loss_cos * alpha_cos\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.0\n",
    "learning_rate = 5e-4\n",
    "adam_epsilon = 1e-6\n",
    "\n",
    "warmup_prop = 0.05\n",
    "gradient_accumulation_steps = 50\n",
    "n_epoch = 1\n",
    "num_steps_epoch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in student_model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in student_model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon, betas=(0.9, 0.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "num_train_optimization_steps = (int(num_steps_epoch / gradient_accumulation_steps * n_epoch) + 1)\n",
    "warmup_steps = math.ceil(num_train_optimization_steps * warmup_prop)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=num_train_optimization_steps,\n",
    ")\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing to new code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fastdistillation import distiller\n",
    "from src.fastdistillation import lm_seqs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [\n",
    "    tokenizer.unk_token_id,\n",
    "    tokenizer.sep_token_id,\n",
    "    tokenizer.pad_token_id,\n",
    "    tokenizer.cls_token_id,\n",
    "    tokenizer.mask_token_id,\n",
    "]\n",
    "token_name_id_map = dict(zip(tokenizer.special_tokens_map.keys(), token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/01/2022 21:48:50 - INFO - src.fastdistillation.utils - PID: 418134 -  Initializing Distiller\n",
      "09/01/2022 21:48:50 - INFO - src.fastdistillation.utils - PID: 418134 -  Using MLM loss for LM step.\n",
      "09/01/2022 21:48:50 - INFO - src.fastdistillation.utils - PID: 418134 -  --- Initializing model optimizer\n",
      "09/01/2022 21:48:50 - INFO - src.fastdistillation.utils - PID: 418134 -  ------ Number of trainable parameters (student): 66032482\n",
      "09/01/2022 21:48:50 - INFO - src.fastdistillation.utils - PID: 418134 -  ------ Number of parameters (student): 66425698\n",
      "09/01/2022 21:48:50 - INFO - src.fastdistillation.utils - PID: 418134 -  --- Initializing Tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Distiller Params\n",
    "from types import SimpleNamespace\n",
    "\n",
    "params = SimpleNamespace(**{\n",
    "    \"dump_path\": \"test.pth\",\n",
    "    \"batch_size\": 5,\n",
    "    \"temperature\": 2.0,\n",
    "    \"alpha_ce\": 0.33,\n",
    "    \"alpha_mlm\": 0.33,\n",
    "    \"alpha_cos\": 0.33,\n",
    "\n",
    "    \"mlm_mask_prop\": 0.15,\n",
    "    \"word_mask\": 0.8,\n",
    "    \"word_keep\": 0.1,\n",
    "    \"word_rand\": 0.1,\n",
    "\n",
    "    \"gradient_accumulation_steps\": 50,\n",
    "    \"n_epoch\": 1,\n",
    "    \"max_grad_norm\": 5.0,\n",
    "\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"adam_epsilon\": 1e-6,\n",
    "\n",
    "    \"warmup_prop\": 0.05,\n",
    "    \"special_tok_ids\": token_name_id_map,\n",
    "\n",
    "    \"device\": \"cuda\",\n",
    "    \"log_interval\": 500,\n",
    "    \"checkpoint_interval\": 4000,\n",
    "})\n",
    "\n",
    "\n",
    "# Dataset Params\n",
    "dataset_data = batch[0]\n",
    "dataset_params = SimpleNamespace(**{\n",
    "    \"special_tok_ids\": tokenizer.special_tokens_map\n",
    "})\n",
    "\n",
    "dataset = lm_seqs_dataset.LmSeqsDataset(dataset_data, attn_mask)\n",
    "token_probs = token_probs\n",
    "student = student_model\n",
    "teacher = teacher_model\n",
    "\n",
    "dist = distiller.Distiller(\n",
    "    params,\n",
    "    dataset,\n",
    "    token_probs,\n",
    "    student,\n",
    "    teacher,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/01/2022 21:48:50 - INFO - src.fastdistillation.utils - PID: 418134 -  Starting training\n",
      "09/01/2022 21:48:50 - INFO - src.fastdistillation.utils - PID: 418134 -  --- Starting epoch 0/0\n",
      "-Iter: 100%|██████████| 205/205 [01:28<00:00,  2.30it/s, Last_loss=4.25, Avg_cum_loss=4.63]\n",
      "09/01/2022 21:50:19 - INFO - src.fastdistillation.utils - PID: 418134 -  --- Ending epoch 0/0\n",
      "09/01/2022 21:50:19 - INFO - src.fastdistillation.utils - PID: 418134 -  1024 sequences have been trained during this epoch.\n",
      "09/01/2022 21:50:21 - INFO - src.fastdistillation.utils - PID: 418134 -  Save very last checkpoint as `pytorch_model.bin`.\n",
      "09/01/2022 21:50:22 - INFO - src.fastdistillation.utils - PID: 418134 -  Training is finished\n"
     ]
    }
   ],
   "source": [
    "dist.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('compression-text-models-fmgxBqLT-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f100f66c9b41ceea0e3b5f026313cb1e6469ea04468aa1f9f63af1de251c63a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
